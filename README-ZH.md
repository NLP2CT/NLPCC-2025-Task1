# <p align="center"><font size=50><strong>NLPCC2025 共享任务 1: 大型语言模型生成文本检测</strong></font></p> 

<div style="text-align: center;">
    <a href="https://github.com/NLP2CT/NLPCC-2025-Task1/blob/main/README.md">English HomePage</a>
</div>

# 任务介绍

随着大型语言模型的迅速发展，其生成文本的质量正逐步接近人工撰写的水平。然而，这些模型也带来了诸多挑战，例如可能生成虚假信息、有害内容，或被用于不当用途。因此，如何有效区分大型语言模型生成的文本与人工撰写的文本，已成为一个重要且紧迫的问题。尽管在检测大型语言模型生成文本方面已有显著进展，但相关研究主要集中在英语领域。相比之下，针对中文的研究仍显得相对不足。本次共享任务旨在弥补这一空白，通过开发更强大的检测算法来识别大型语言模型生成的中文文本，从而推动中文领域相关研究的深入发展。

参赛者需要基于提供的原始训练数据，设计并构建检测算法，用以区分大型语言模型生成的文本和人工撰写的文本。在评估阶段，所有提交的检测器将在模拟真实场景的测试条件下（尤其是分布外数据的情况下）进行严格测试，以全面评估其实际效果和鲁棒性。为确保公平性和结果可追溯性，参赛者禁止使用外部数据源或基于外部知识生成新的数据样本。此外，所有训练数据和相关脚本需提交进行审查，以保证任务的公平性、透明性和可复现性。

# 最新消息

- [ 2025.02.27 ] 我们已发布详尽的任务指南和训练数据，快来着手构建属于你自己的高效可靠的检测器吧！
- [ 2025.04.11 ] 📢 NLPCC-2025 Task 1 的测试数据已经发布！

**PS: 我们已向所有参赛队伍发送了测试数据发布的相关通知邮件。如果您已注册参赛但未收到相关通知，请尽快通过邮件与我们联系，避免参赛信息遗漏。**

# 数据描述

我们推出了一个名为 DetectRL-ZH 的中文基准测试集，用于检测大语言模型生成的中文文本。该测试集是英文基准测试集 DetectRL 的中文扩展版本。DetectRL-ZH 精心构建了一个模拟现实世界场景中生成文本的数据集，涵盖多样化的内容类型，包括多种释义、对抗性样本以及数据混合样本，从而能够更全面地反映现实中的复杂应用场景。为确保评估结果更具实际意义，我们的测试集设计和评估将同样均基于贴近真实世界的测试场景。以下是本次共享任务所提供数据集的详细统计信息。

## 数据统计

- 训练集：训练集包含来自3种大语言模型和3个领域的数据。具体来说，数据来源包括 ASAP（代表社交媒体评论）、CNewSum（代表新闻写作） 和 CSL（代表学术写作），生成模型包括 GPT-4o、GLM-4-flash 和 Qwen-turbo。训练集总共包含 31,200 个样本。
- 测试集：最终测试使用额外的未知模型和领域数据作为原始数据来源。

| Split | Source    | GPT4o | GLM  | Qwen  | Machine | Human | Total |
|-------|-----------|-------|------|-------|---------|-------|-------|
| Train | ASAP      | 2700  | 2700 | 2700  | 8100    | 2700  | 32400 |
|       | CNewSum   | 2700  | 2700 | 2700  | 8100    | 2700  |       |
|       | CSL       | 2700  | 2700 | 2700  | 8100    | 2700  |       |
| Dev   | -    |   -   | -    | -     | 1700    | 1100  |  2800 |
| Test  | -    | -     | -    | -     | -       | -     | -     |


## 数据下载

训练数据和开发数据可以在以下 Google Drive 或者 Github 文件夹链接中找到：

- Google Drive link: https://drive.google.com/drive/folders/1R5KiW7uwQ002dOE2expEYQLbzQ_gMr8j?usp=sharing

- Github link: https://github.com/NLP2CT/NLPCC2025-Task1/tree/main/data

##  数据限制

- 请注意，所提供的开发集仅限用于模型调优，不允许用于模型训练。

- 为了支持检测系统的开发，允许参赛者基于提供的原始训练数据进行数据增强。然而，数据增强的范围必须严格限定在对原始数据的处理或转换，例如通过裁剪、拆分、词语替换或格式调整等方法生成新的数据样本。所有操作必须确保严格保留原始数据的语义，不得引入任何外部知识或生成完全新创的内容。

- 特别需要强调的是，禁止使用生成式大型语言模型（Generative LLM）进行释义，因为这可能无意间引入分布外知识，从而导致不公平的优势。然而，允许使用传统的编码器模型或序列到序列（seq2seq）模型进行释义操作，前提是严格遵守语义保留的要求。

## 数据格式

数据以 JSON 对象的格式进行存储。

- 训练数据样式:
```json
{
  "text": "text generated by a machine or written by a human",
  "label": "label (human text: 0, machine text: 1)",
  "model": "model that generated the data",
  "source": "source (ASAP, CNewSum, CSL)"
}
```

- 开发集和测试集样式:
```json
{
  "text": "text generated by a machine or written by a human",
  "label": "label (human text: 0, machine text: 1)",
}
```

# 评估指标

该任务的官方评估指标是 macro-averaged F1-Score。

# 提交与评估

## 提交要求

提交结果时，请将以下材料打包成一个zip文件发送至nlp2ct.junchao@gmail.com：

1. 测试结果文件
- 您的测试结果文件必须为一个包含所有样本的 JSON 文件。
- 请确保 text 和 id 字段保持原样，不得进行任何修改。
- JSON 文件中每条样本需包含以下字段：
    - "id"：样本的唯一标识符
    - "text"：样本的文本内容
    - "label"：分类结果（请根据以下规则标注）：
        - 人类写作文本：标注为 0
        - 大语言模型生成文本：标注为 1
      
2. 代码和数据
- 代码文件夹应包含数据增强、数据处理、模型训练和模型推理的所有代码，以及用于训练您的检测器的完整数据集（模型训练时禁止使用违规数据）。
- 由于提交的代码可能会被我们检查并复现，因此请提供一个简单的README.md（或等效文档）和环境配置文件requirements.txt（如有），并在文档中中简单说明代码复现的流程，确保能够复现您所提交的实验结果。

3. 技术简要报告
- 技术简要报告文档应详细说明所提交的用于解决该任务的方法，包括数据处理、数据增强以及所采用的方法/模型架构和具体的各项参数。如有必要，可以提供公式和伪代码以方便理解。

！请注意，本次Shared Task没有在任何第三方平台组织实时排行榜用于打榜。所有的参赛队伍在提交截止时间前可以无限次提交结果，我们将以最后一次测试结果作为最终结果来计算成绩。另外，我们的最终评估指标将采用 macro-averaged F1 score。

## 重要日期

- 2025 年 4 月 20 日：测试结果提交截止日期
- 2025 年 4 月 30 日：评测结果公布

## 注意事项

- 如发现违反比赛规则的行为，参赛成绩将被取消。
- 截止日期之后提交的成绩将不被受理。
- 如果在比赛过程中遇到任何问题，欢迎通过以下邮箱联系我们：nlp2ct.junchao@gmail.com。

# 重要日期  

| 时间         | 事件                                         |  
| ------------ | -------------------------------------------- |  
| 2025/02/17   | 公布共享任务并发布参赛邀请                   |  
| 2025/02/17   | 开放注册                                     |  
| 2025/02/28   | 发布详细任务指南及训练数据                   |  
| 2025/03/25   | 注册截止                                     |  
| 2025/04/11   | 发布测试数据                                 |  
| 2025/04/20   | 参赛者提交结果的截止日期                     |  
| 2025/04/30   | 公布评估结果并发布系统报告及会议论文邀请     |  
| 2025/05/22   | 会议论文提交截止日期（仅限共享任务）         |  
| 2025/06/12   | 会议论文接收/拒绝通知                        |  
| 2025/06/25   | 终稿论文提交截止日期                         |  

**时间说明：所有时间均为北京时间当天晚23:59。**

# 奖项

- 任务的前三名参赛队伍将由 NLPCC 和 CCF-NLP 授予证书。

# 主办方 & 联系方式

本次共享任务由澳门大学自然语言处理与中葡机器翻译实验室（NLP2CT Lab）组织。

- [黄辉（Derek, Fai Wong）](https://www.fst.um.edu.mo/personal/derek-wong/)
- [吴俊潮（Junchao Wu）](https://junchaoiu.github.io/)
- [詹润哲（Runzhe Zhan）](https://runzhe.me/) 
- [袁毓林（Yulin Yuan）](https://fah.um.edu.mo/yulin-yuan/)

如果您对本次任务有任何疑问，请发送邮件至 nlp2ct.junchao@gmail.com。

# 常见问题

问: 在哪里可以注册/报名参加本次共享任务？

答: 最新的注册方式可在 NLPCC 2025 共享任务官网（http://tcci.ccf.org.cn/conference/2025/cfpt.php ）上找到。请按照要求填写共享任务 1 的注册表（Word 文档）(http://tcci.ccf.org.cn/conference/2025/sharedTasks/NLPCC2025.SharedTask1.RegistrationForm.doc )，并发送至 nlp2ct.junchao@gmail.com。如有任何疑问，请随时联系。

问: 是否允许使用额外的数据？

答： 不允许使用外部数据源，但允许数据增强（详情请参阅数据限制部分）。

问: 任务官网（GitHub）与会议官网关于“数据限制”的描述不一致：
- GitHub：禁止使用生成式大型语言模型（Generative LLM）进行释义，仅允许使用传统编码器模型或序列到序列（seq2seq）模型。
- 会议官网：http://tcci.ccf.org.cn/conference/2025/cfpt.php 允许开源模型和API模型，但禁止GPT-o1、MoE或参数规模超过80B的模型。

答: 请以 GitHub 官网（https://github.com/NLP2CT/NLPCC-2025-Task1）上的规则为准：
- 仅允许 使用传统编码器模型或序列到序列（seq2seq）模型进行释义操作。
- 严禁 使用任何生成式解码器模型（即使参数小于80B，如glm-4-9B）。
- 这一限制是为了避免生成式模型对含有解码器模型数据的测试集造成潜在不公平。

问: “释义”是否意味着任务全流程中都不能使用大语言模型？

答: 
- 释义定义：对文本进行语义等价的修改，包括润色、同义词替换、表达方式调整等。
- 释义规则：数据增强阶段（如释义操作）中，仅允许 使用编码器或seq2seq模型，禁止 使用任何生成式大语言模型（包括文本润色或修订）。
- 任务其他阶段：在数据增强外的全流程中，可以使用大语言模型（包括微调LLM完成任务）。

问: 在不使用LLM进行释义的情况下，是否可以使用LLM微调模型（如qwen2.5-3B）作为检测器？

答: 数据增强阶段禁止使用大语言模型。检测器构建阶段允许使用大语言模型，包括：
- 直接微调大语言模型完成任务；
- 使用开源模型提取内部特征辅助检测。

问: 仅使用编码器模型进行微调是否符合规则？

答: 允许使用任何方法构建检测器，包括：
- 微调编码器模型或解码器模型作为分类器；
- 使用统计方法提取分类特征等。
- 限制仅针对数据增强过程（不能用生成式大模型释义文本）。

问: 开发集只能用于“调优”，不能用于“训练”。两者有何区别？

答:
- 训练（Training）：使用训练集（train.json）调整模型内部参数（如神经网络权重），或从训练集中提取分类特征和阈值。
- 调优（Tuning）：使用开发集（dev.json）仅在训练完成后调整超参数（如学习率）、分类特征或模型架构。用于评估检测器性能并优化方法，以提高模型在未见数据上的表现。
- 举例：如果您在训练集上微调了一个编码器模型，但在开发集上表现不佳，这可能是过拟合或鲁棒性欠佳，需要采取更多的措施（如数据增强或者提取更多样化的特征）来提高方法的泛化能力，以更好地适应分布外测试场景。。

问: 参赛代码需要在何时提交？提交到哪里？

答: 
- 提交时间：4月20日及其之前（北京时间23:59）。
- 提交内容：测试结果文件、训练数据、源代码（包括readme.md或运行说明文档）、技术简要报告。
- 提交地点：发送至邮箱 nlp2ct.junchao@gmail.com。
- 倘若参赛规则有更改，将通过 GitHub 官方网站 通知。

问: 所有团队成员是否需要单独注册？企业成员是否可以参赛？

答: 一个团队只需提交一份注册表，无需重复注册。企业成员可以参赛。

问: 测试数据发布后，是否仅需提交测试结果 *.json 文件？

答: 需提交的文件：
- 最终测试结果文件（*.json）；
- 检测器数据与源代码（用于验证方法）；
- 技术简要报告；
- 如需额外材料，组织方将通过邮件和官网通知。

# 参考文献

如果您是该领域的新的研究人员，我们希望以下论文可以帮助您快速熟悉该领域（持续更新中）：

- Wu, J., Yang, S., Zhan, R., Yuan, Y., Chao, L. S., & Wong, D. F. (2025). A survey on LLM-generated text detection: Necessity, methods, and future directions. Computational Linguistics, 1-66.
- Wu, J., Zhan, R., Wong, D. F., Yang, S., Yang, X., Yuan, Y., & Chao, L. S. (2024). DetectRL: Benchmarking LLM-Generated Text Detection in Real-World Scenarios. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track.
- Mitchell, E., Lee, Y., Khazatsky, A., Manning, C. D., & Finn, C. (2023, July). Detectgpt: Zero-shot machine-generated text detection using probability curvature. In International Conference on Machine Learning (pp. 24950-24962). PMLR.
- Bao, G., Zhao, Y., Teng, Z., Yang, L., & Zhang, Y. Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature. In The Twelfth International Conference on Learning Representations.
- Hans, A., Schwarzschild, A., Cherepanova, V., Kazemi, H., Saha, A., Goldblum, M., ... & Goldstein, T. (2024, July). Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated Text. In International Conference on Machine Learning (pp. 17519-17537). PMLR.
- Guo, B., Zhang, X., Wang, Z., Jiang, M., Nie, J., Ding, Y., ... & Wu, Y. (2023). How close is chatgpt to human experts? comparison corpus, evaluation, and detection. arXiv preprint arXiv:2301.07597.
- Wu, J., Zhan, R., Wong, D. F., Yang, S., Liu, X., Chao, L. S., & Zhang, M. (2025, January). Who Wrote This? The Key to Zero-Shot LLM-Generated Text Detection Is GECScore. In Proceedings of the 31st International Conference on Computational Linguistics (pp. 10275-10292).
